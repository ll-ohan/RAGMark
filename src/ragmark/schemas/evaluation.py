"""Evaluation data models.

This module defines data structures for benchmark trial cases,
evaluation results, and audit reports.
"""

import json
from datetime import datetime, timezone
from pathlib import Path
from typing import Any
from uuid import uuid4

import pandas as pd
from pydantic import BaseModel, ConfigDict, Field, field_validator

from ragmark.schemas.generation import GenerationResult
from ragmark.schemas.retrieval import TraceContext


class TrialCase(BaseModel):
    """A single test case for benchmarking.

    Each trial case represents a question with optional ground truth
    for evaluating either retrieval quality, generation quality, or both.

    Attributes:
        case_id: Unique identifier for this test case.
        question: The question or query to evaluate.
        ground_truth_answer: Expected answer for generation evaluation.
        ground_truth_node_ids: Expected node IDs for retrieval evaluation.
        metadata: Additional metadata (difficulty, category, etc.).
    """

    model_config = ConfigDict(strict=True, extra="forbid")

    case_id: str = Field(
        default_factory=lambda: str(uuid4()),
        description="Unique case identifier",
    )
    question: str = Field(..., description="The question or query to evaluate")
    ground_truth_answer: str | None = Field(
        None,
        description="Expected answer for generation evaluation",
    )
    ground_truth_node_ids: list[str] | None = Field(
        None,
        description="Expected node IDs for retrieval evaluation",
    )
    metadata: dict[str, str | int | float | dict[str, Any] | None] = Field(
        default_factory=dict,
        description="Additional metadata (difficulty, category, etc.)",
    )

    @field_validator("question")
    @classmethod
    def question_not_empty(cls, v: str) -> str:
        """Validate that question is not empty.

        Args:
            v: The question to validate.

        Returns:
            The validated question.

        Raises:
            ValueError: If question is empty.
        """
        if not v or not v.strip():
            raise ValueError("Question cannot be empty")
        return v

    def model_post_init(self, __context: Any) -> None:
        """Validate that at least one ground truth is provided.

        Raises:
            ValueError: If neither ground_truth_answer nor ground_truth_node_ids is provided.
        """
        if self.ground_truth_answer is None and self.ground_truth_node_ids is None:
            raise ValueError(
                "At least one of ground_truth_answer or ground_truth_node_ids must be provided"
            )

    @classmethod
    def load_cases(cls, path: Path) -> list["TrialCase"]:
        """Load trial cases from a JSON or JSONL file.

        Args:
            path: Location of the file containing trial cases.

        Returns:
            The loaded trial cases.

        Raises:
            ValueError: If file format is not supported.
            FileNotFoundError: If file does not exist.
        """
        if not path.exists():
            raise FileNotFoundError(f"Trial cases file not found: {path}")

        cases: list[TrialCase] = []
        if path.suffix == ".jsonl":
            with open(path, encoding="utf-8") as f:
                for line in f:
                    if line.strip():
                        cases.append(cls.model_validate_json(line))
        elif path.suffix == ".json":
            with open(path, encoding="utf-8") as f:
                data: list[dict[str, Any]] | dict[str, Any] = json.load(f)
                if isinstance(data, list):
                    cases = [cls.model_validate(item) for item in data]
                else:
                    cases = [cls.model_validate(data)]
        else:
            raise ValueError(f"Unsupported file format: {path.suffix}")

        return cases


class CaseResult(BaseModel):
    """Evaluation results for a single trial case.

    This captures the system's response to a trial case along with
    computed metrics.

    Attributes:
        case_id: Reference to the TrialCase.
        predicted_answer: The answer generated by the system.
        trace: Complete retrieval trace.
        generation_result: LLM generation details.
        case_metrics: Computed metrics for this case (recall@k, faithfulness, etc.).
    """

    model_config = ConfigDict(strict=True, extra="forbid")

    case_id: str = Field(..., description="Reference to the TrialCase")
    predicted_answer: str | None = Field(
        None,
        description="Answer generated by the system",
    )
    trace: TraceContext = Field(..., description="Complete retrieval trace")
    generation_result: GenerationResult | None = Field(
        None,
        description="LLM generation details",
    )
    case_metrics: dict[str, float] = Field(
        default_factory=dict,
        description="Computed metrics for this case",
    )


class SystemInfo(BaseModel):
    """System information for reproducibility.

    Captures hardware and software environment details.

    Attributes:
        python_version: Python interpreter version.
        ragmark_version: RAGMark library version.
        platform: Operating system and architecture.
        cpu_count: Number of CPU cores.
        ram_gb: Total RAM in gigabytes.
        gpu_info: GPU information if available.
    """

    model_config = ConfigDict(strict=True, extra="forbid")

    python_version: str = Field(..., description="Python interpreter version")
    ragmark_version: str = Field(..., description="RAGMark library version")
    platform: str = Field(..., description="Operating system and architecture")
    cpu_count: int = Field(..., ge=1, description="Number of CPU cores")
    ram_gb: float = Field(..., gt=0, description="Total RAM in gigabytes")
    gpu_info: str | None = Field(None, description="GPU information if available")


class AuditReport(BaseModel):
    """Complete audit report for a benchmarking campaign.

    This model aggregates all results, metrics, and metadata from
    a full benchmark run.

    Attributes:
        report_id: Unique identifier for this report.
        experiment_profile_hash: Hash of the experiment configuration.
        created_at: Timestamp when the report was created.
        duration_seconds: Total duration of the benchmark.
        metrics: Aggregated metrics (recall@5, mrr, faithfulness, etc.).
        per_case_results: Individual results for each trial case.
        system_info: System information for reproducibility.
    """

    model_config = ConfigDict(strict=True, extra="forbid")

    report_id: str = Field(
        default_factory=lambda: str(uuid4()),
        description="Unique report identifier",
    )
    experiment_profile_hash: str = Field(
        ...,
        description="Hash of the experiment configuration",
    )
    created_at: datetime = Field(
        default_factory=lambda: datetime.now(timezone.utc),
        description="Report creation timestamp",
    )
    duration_seconds: float = Field(..., ge=0, description="Total benchmark duration")
    metrics: dict[str, float] = Field(
        default_factory=dict,
        description="Aggregated metrics",
    )
    per_case_results: list[CaseResult] = Field(
        default_factory=list[CaseResult],
        description="Individual case results",
    )
    system_info: SystemInfo = Field(..., description="System information")

    def to_json(self, path: Path, indent: int = 2) -> None:
        """Export report to JSON file.

        Args:
            path: Destination for the output file.
            indent: JSON indentation level.
        """
        with open(path, "w", encoding="utf-8") as f:
            f.write(self.model_dump_json(indent=indent))

    def to_dataframe(self) -> pd.DataFrame:
        """Convert per-case results to a pandas DataFrame.

        Returns:
            A table containing one row per case result.
        """
        rows: list[dict[str, Any]] = []
        for result in self.per_case_results:
            row: dict[str, bool | str | int | float | None] = {
                "case_id": result.case_id,
                "predicted_answer": result.predicted_answer,
                "query": result.trace.query,
                "num_retrieved": len(result.trace.retrieved_nodes),
                "reranked": result.trace.reranked,
            }
            row.update(result.case_metrics)
            if result.generation_result:
                gen_result = result.generation_result
                row["prompt_tokens"] = gen_result.usage.prompt_tokens
                row["completion_tokens"] = gen_result.usage.completion_tokens
                row["finish_reason"] = gen_result.finish_reason
            rows.append(row)

        return pd.DataFrame(rows)
